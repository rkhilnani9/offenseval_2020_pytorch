{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-13T10:12:11.097106Z",
     "start_time": "2020-01-13T10:12:06.880558Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformers import *\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import codecs\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from scipy import sparse\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from math import log\n",
    "import operator\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import math\n",
    "import spacy\n",
    "torch.manual_seed(123)\n",
    "\n",
    "import random\n",
    "random.seed(123)\n",
    "\n",
    "np.random.seed(123)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import codecs\n",
    "import string\n",
    "from scipy import sparse\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import math\n",
    "import torch.utils.data as data_utils\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_class, tokenizer_class, config_class, pretrained_weights = BertModel, BertTokenizer, BertConfig, 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pretrained_weights = 'bert-base-uncased'\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights,\n",
    "                                            do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(path, avg):\n",
    "    df = pd.read_table(path, sep = '\\t')\n",
    "    df['label'] = np.where(df['average'] > avg, 1, 0)\n",
    "    x = df['text']\n",
    "    y = df['label']\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    corpus, dev_corpus, y, dev_y = train_test_split(x, y, test_size = 0.2, random_state = 42)\n",
    "    return corpus.values, y.values, dev_corpus.values, dev_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'task_a_distant.tsv'\n",
    "avg = 0.5\n",
    "corpus, y, dev_corpus, dev_y = load_data(df, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_tokenized = corpus\n",
    "dev_corpus_tokenized = dev_corpus\n",
    "for i in range(len(corpus)):\n",
    "    try:\n",
    "        corpus_tokenized[i] = tokenizer.tokenize(corpus[i])\n",
    "        if (i%10000 == 0):\n",
    "            print (i, len(corpus))\n",
    "    except:\n",
    "        print (i)\n",
    "        pass\n",
    "    \n",
    "for i in range(len(dev_corpus)):\n",
    "    try:\n",
    "        dev_corpus_tokenized[i] = tokenizer.tokenize(dev_corpus[i])\n",
    "        if (i%10000 == 0):\n",
    "            print (i, len(dev_corpus))\n",
    "    except:\n",
    "        print (i)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "length_ = 0\n",
    "for i in range(10000):\n",
    "    length_ += len(corpus_tokenized[i])\n",
    "\n",
    "print (\"Average Length: \", length_/10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(corpus):\n",
    "    input_ids_list = []\n",
    "    segment_ids_list = []\n",
    "    input_mask_list = []\n",
    "    max_seq_length = 64\n",
    "    for i in range(len(corpus)):\n",
    "        to_append = [\"[CLS]\"] + corpus[i][:max_seq_length-2] + [\"[SEP]\"] \n",
    "        segment_ids = [0] * (len(corpus[i][:max_seq_length-2]) + 2) \n",
    "        input_ids = tokenizer.convert_tokens_to_ids(to_append)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "        assert (len(input_ids) == max_seq_length)\n",
    "        assert (len(input_mask) == max_seq_length)\n",
    "        assert (len(segment_ids) == max_seq_length)\n",
    "        input_ids_list.append(input_ids)\n",
    "        segment_ids_list.append(segment_ids)\n",
    "        input_mask_list.append(input_mask)\n",
    "\n",
    "    return input_ids_list, segment_ids_list, input_mask_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize_dev(corpus):\n",
    "    input_ids_list = []\n",
    "    segment_ids_list = []\n",
    "    input_mask_list = []\n",
    "    max_seq_length = 64\n",
    "    for i in range(len(corpus)):\n",
    "        to_append = [\"[CLS]\"] + corpus[i][:max_seq_length-2] + [\"[SEP]\"] \n",
    "        segment_ids = [0] * (len(corpus[i][:max_seq_length-2]) + 2) \n",
    "        input_ids = tokenizer.convert_tokens_to_ids(to_append)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "        assert (len(input_ids) == max_seq_length)\n",
    "        assert (len(input_mask) == max_seq_length)\n",
    "        assert (len(segment_ids) == max_seq_length)\n",
    "        input_ids_list.append(input_ids)\n",
    "        segment_ids_list.append(segment_ids)\n",
    "        input_mask_list.append(input_mask)\n",
    "\n",
    "    return input_ids_list, segment_ids_list, input_mask_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_ids_list, segment_ids_list, input_mask_list = vectorize(corpus_tokenized)\n",
    "input_ids_list2, segment_ids_list2, input_mask_list2 = vectorize_dev(dev_corpus_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.array(y)\n",
    "dev_y = np.array(dev_y)\n",
    "\n",
    "y = y[..., np.newaxis]\n",
    "dev_y = dev_y[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_ids_list, segment_ids_list, input_mask_list = np.array(input_ids_list), np.array(segment_ids_list), np.array(input_mask_list)\n",
    "input_ids_list2, segment_ids_list2, input_mask_list2 = np.array(input_ids_list2), np.array(segment_ids_list2), np.array(input_mask_list2)\n",
    "\n",
    "train_dset = data_utils.TensorDataset(torch.from_numpy(input_ids_list).to(device), torch.from_numpy(segment_ids_list).to(device), torch.from_numpy(input_mask_list).to(device), torch.from_numpy(y_new).to(device))\n",
    "train_loader = data_utils.DataLoader(\n",
    "    train_dset,\n",
    "    batch_size=32, shuffle = True\n",
    ")\n",
    "\n",
    "val_dset = data_utils.TensorDataset(torch.from_numpy(input_ids_list2).to(device), torch.from_numpy(segment_ids_list2).to(device), torch.from_numpy(input_mask_list2).to(device), torch.from_numpy(dev_y_new).to(device))\n",
    "val_loader = data_utils.DataLoader(\n",
    "    val_dset,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, bert_model, dropout_p = 0.1):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.classifier = nn.Linear(768,2)\n",
    "        self.classifier.weight.data.normal_(mean=0.0, std=0.02)        \n",
    "\n",
    "    def forward(self, input_ids, segment_ids, input_mask, embeds_only = False, embeds = None):\n",
    "        x1 = embeds\n",
    "        if not embeds_only:\n",
    "            x1 = self.bert(input_ids, token_type_ids = segment_ids, attention_mask = input_mask)[0]\n",
    "            x1 = x1[:,0]\n",
    "        drop_x1 = self.dropout(x1)\n",
    "        y = self.classifier(drop_x1) # return logits\n",
    "        y  = torch.log_softmax(y, dim = 1)\n",
    "        return y, x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, loss_function, optimizer, epoch_num):\n",
    "    from sklearn.metrics import f1_score\n",
    "    model.train() \n",
    "    avg_loss = 0.0\n",
    "    count = 0\n",
    "    truth_res = []\n",
    "    pred_res = []\n",
    "    pred_probs_list_train = []\n",
    "    for input_ids, segment_ids, input_mask, label in dataloader:\n",
    "        input_ids, segment_ids, input_mask, label = input_ids.to(\"cuda\"), segment_ids.to(\"cuda\"), input_mask.to(\"cuda\"), label.to(\"cuda\")\n",
    "        model.to(device)\n",
    "        pred = model(input_ids, segment_ids, input_mask)\n",
    "        pred_prob = pred[:, 1].detach().data.cpu().numpy()\n",
    "        pred_probs_list_train.append(np.exp(pred_prob))\n",
    "        model.zero_grad()\n",
    "        loss = loss_function(pred, label.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pred_label = pred.data.max(1)[1].cpu()\n",
    "        pred_res += [pred_label]\n",
    "        truth_res += [label.detach().data.cpu()]\n",
    "        avg_loss += loss.detach().data.item()\n",
    "        count += 1\n",
    "        if count % 5000 == 0:\n",
    "            print('[TRAIN] epoch: %d iterations: %d loss :%g' % (epoch_num, count, loss.detach().data.item()))\n",
    "\n",
    "    avg_loss /= len(input_ids_list)\n",
    "    print('[TRAIN] epoch: %d done! \\n train avg_loss:%g , f1:%g'%(epoch_num, avg_loss, f1_score(torch.cat(truth_res),torch.cat(pred_res), average = 'macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_epoch(model, dataloader, loss_function, optimizer, epoch_num):\n",
    "    from sklearn.metrics import f1_score\n",
    "    model.eval()\n",
    "    #avg_loss = 0.0\n",
    "    count = 0\n",
    "    truth_res = []\n",
    "    pred_res = []\n",
    "    pred_probs_list = []\n",
    "    for input_ids, segment_ids, input_mask in dataloader:\n",
    "        input_ids, segment_ids, input_mask = input_ids.to(\"cuda\"), segment_ids.to(\"cuda\"), input_mask.to(\"cuda\")\n",
    "        model.to(device)\n",
    "        pred = model(input_ids, segment_ids, input_mask)\n",
    "        #loss = loss_function(pred, label.view(-1))\n",
    "        pred_prob = pred[:, 1].detach().data.cpu().numpy()\n",
    "        pred_probs_list.append(np.exp(pred_prob))\n",
    "        #pred_probs = np.argmax(pred_probs, axis=1)\n",
    "        #pred_probs_list += [pred_probs]\n",
    "        pred_label = pred.data.max(1)[1].cpu()\n",
    "        pred_res += [pred_label]\n",
    "        #truth_res += [label.detach().data.cpu()]\n",
    "        #avg_loss += loss.detach().data.item()\n",
    "        #count += 1\n",
    "    #avg_loss /= len(input_ids_list)\n",
    "    print('[EVAL] epoch: %d done!'%(epoch_num))\n",
    "    return pred_probs_list, pred_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "epoch = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    train_epoch(model, train_loader, loss_function, optimizer, epoch)\n",
    "    pred_probs_list, pred_res = eval_epoch(model, val_loader, loss_function, optimizer, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_table('/rapids/notebooks/atcdata/SEV T12/test_data/offenseval-tr-testset-v1.tsv', sep = '\\t')\n",
    "test_corpus = test_data['tweet'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_corpus_tokenized = test_corpus\n",
    "for i in range(len(test_corpus)):\n",
    "    try:\n",
    "        test_corpus_tokenized[i] = tokenizer.tokenize(test_corpus[i])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize_test(corpus):\n",
    "    input_ids_list = []\n",
    "    segment_ids_list = []\n",
    "    input_mask_list = []\n",
    "    max_seq_length = 512\n",
    "    for i in range(len(corpus)):\n",
    "        to_append = [\"[CLS]\"] + corpus[i] + [\"[SEP]\"] \n",
    "        segment_ids = [0] * (len(corpus[i]) + 2) \n",
    "        input_ids = tokenizer.convert_tokens_to_ids(to_append)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "        assert (len(input_ids) == max_seq_length)\n",
    "        assert (len(input_mask) == max_seq_length)\n",
    "        assert (len(segment_ids) == max_seq_length)\n",
    "        input_ids_list.append(input_ids)\n",
    "        segment_ids_list.append(segment_ids)\n",
    "        input_mask_list.append(input_mask)\n",
    "        \n",
    "\n",
    "    return input_ids_list, segment_ids_list, input_mask_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_ids_list_test, segment_ids_list_test, input_mask_list_test = vectorize_test(test_corpus_tokenized) \n",
    "input_ids_list_test, segment_ids_list_test, input_mask_list_test = np.array(input_ids_list_test), np.array(segment_ids_list_test), np.array(input_mask_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dset = data_utils.TensorDataset(torch.from_numpy(input_ids_list_test).to(device), torch.from_numpy(segment_ids_list_test).to(device), torch.from_numpy(input_mask_list_test).to(device))\n",
    "test_loader = data_utils.DataLoader(\n",
    "    test_dset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = ClassifierDBert(bert_model, 0.6)\n",
    "test_model.load_state_dict(torch.load('bert_simple_1.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_epoch(model, dataloader, optimizer):\n",
    "    from sklearn.metrics import f1_score\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    truth_res = []\n",
    "    pred_res = []\n",
    "    for input_ids, segment_ids, input_mask in dataloader:\n",
    "        input_ids, segment_ids, input_mask = input_ids.to(\"cuda\"), segment_ids.to(\"cuda\"), input_mask.to(\"cuda\")\n",
    "        model.to(device)\n",
    "        pred = model(input_ids, segment_ids, input_mask)\n",
    "        pred_label = pred.data.max(1)[1].cpu()\n",
    "        pred_res += [pred_label]\n",
    "    return pred_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = test_epoch(test_model, test_loader, optimizer)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
